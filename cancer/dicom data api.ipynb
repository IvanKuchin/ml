{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "from pydicom.data import get_testdata_files\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import shutil\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"/docs/src/kt/data_by_series_resized\"\n",
    "train_folder = \"/docs/src/kt/data_train\"\n",
    "test_folder = \"/docs/src/kt/data_test\"\n",
    "IMG_PX_SIZE = 150\n",
    "IMG_HEIGHT = 150\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "TRAIN_SET_SIZE = 95\n",
    "EPOCHS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllFoldersList(main_folder):\n",
    "    result = []\n",
    "    for patient_folder in glob.glob(main_folder + \"/*\"):\n",
    "        for series_folder in glob.glob(patient_folder + \"/*\"):\n",
    "            result.append(series_folder)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_to_0_1(arr):\n",
    "    rightMin = 0\n",
    "    rightMax = 1\n",
    "    leftMin = arr.min()\n",
    "    leftMax = arr.max()\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    valueScaled = (arr - leftMin) / (leftSpan)\n",
    "\n",
    "    # Convert the 0-1 range into a value in the right range.\n",
    "    return rightMin + (valueScaled * rightSpan)\n",
    "\n",
    "def ScaleImageDownTwice(arr):\n",
    "    arr = np.delete(arr, list(range(0, arr.shape[0], 2)), axis=0)\n",
    "    arr = np.delete(arr, list(range(0, arr.shape[1], 2)), axis=1)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def GetPixelDataFromDCIM(image_file):\n",
    "    _dataset = pydicom.dcmread(image_file)\n",
    "    pixel_array = _dataset.pixel_array\n",
    "    del _dataset\n",
    "    return pixel_array\n",
    "\n",
    "def GetScaledPixelDataFromDCIM(image_file):\n",
    "    return scale_to_0_1(GetPixelDataFromDCIM(image_file))\n",
    "\n",
    "\n",
    "# pix_data = ScaleImageDownTwice(GetScaledPixelDataFromDCIM(root_folder + \"/EFREMOV___SERGEY__ALEXEEVICH_VIPROMID370-85ML/5/1.dcm\"))\n",
    "# print(pix_data)\n",
    "# GetScaledPixelDataFromDCIM(root_folder + \"/EFREMOV___SERGEY__ALEXEEVICH_VIPROMID370-85ML/5/2.dcm  \"  )\n",
    "# GetScaledPixelDataFromDCIM(root_folder + \"/EFREMOV___SERGEY__ALEXEEVICH_VIPROMID370-85ML/5/3.dcm  \"  )\n",
    "# GetScaledPixelDataFromDCIM(root_folder + \"/EFREMOV___SERGEY__ALEXEEVICH_VIPROMID370-85ML/5/4.dcm  \"  )\n",
    "# print(pix_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read3DImageFromFolder(folders):\n",
    "    return np.random.randint(10, size=(IMG_HEIGHT, IMG_PX_SIZE, IMG_PX_SIZE, 1))\n",
    "    result = []\n",
    "    for folder in folders:\n",
    "        file_list = []\n",
    "        folder_str = folder.numpy().decode()\n",
    "#         print(\"data:\", folder_str)\n",
    "        for image in glob.glob(folder_str + \"/*.dcm\"):\n",
    "            image_number = int(image[len(folder_str) + 1:-4])\n",
    "            file_list.append(image_number)\n",
    "        file_list = np.sort(np.array(file_list))\n",
    "        single_3d_photo = []\n",
    "        for file_number in file_list:\n",
    "            file_name = folder_str + \"/\" + str(file_number) + \".dcm\"\n",
    "            pixel_data = (GetScaledPixelDataFromDCIM(file_name))\n",
    "            pixel_data = cv2.resize(np.array(pixel_data),(IMG_PX_SIZE,IMG_PX_SIZE))\n",
    "            single_3d_photo.append(pixel_data)\n",
    "        \n",
    "        result = single_3d_photo\n",
    "    result = np.expand_dims(result, axis=3)\n",
    "    return result[:IMG_HEIGHT, ...]\n",
    "\n",
    "# r1 = Read3DImageFromFolder(tf.constant([\"/docs/src/kt/data_by_series_resized\\\\Znamenskaya N.G. - Body 1.0\\\\3\", \"/docs/src/kt/data_by_series_resized\\\\Znamenskaya N.G. - Body 1.0\\\\12\"]))\n",
    "# r1 = Read3DImageFromFolder(tf.constant([\"/docs/src/kt/data_by_series_resized\\\\Znamenskaya N.G. - Body 1.0\\\\3\"]))\n",
    "# t1 = tf.constant(r1)\n",
    "# t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadCategoryFromFolder(folders):\n",
    "    result = []\n",
    "    for folder in folders:\n",
    "        file_list = []\n",
    "        folder_str = folder.numpy().decode()\n",
    "#         print(\"result:\", folder_str, end=\"\")\n",
    "        single_category = []\n",
    "        file_name = folder_str + \"/result\"\n",
    "        try:\n",
    "            f = open(file_name, \"r\")\n",
    "            result = int(f.read())\n",
    "            f.close()\n",
    "#             print(\" -\", result, end=\"\")\n",
    "        except IOError:\n",
    "            print()\n",
    "            print (\"ERROR: Could not read result_file:\", file_name)\n",
    "#         print()\n",
    "    return result\n",
    "\n",
    "# r1 = ReadCategoryFromFolder(tf.constant([\"/docs/src/kt/data_by_series_resized\\\\Buslaev S.N. - Body 1.0\\\\12\"]))\n",
    "# r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def gen(main_folder):\n",
    "    folders_list = GetAllFoldersList(main_folder.decode())\n",
    "    \n",
    "    folders_dataset = tf.data.Dataset.from_tensor_slices(folders_list).shuffle(buffer_size=100).repeat(EPOCHS).batch(1) # --- batch(1) uses to iterate over Dataset, DO NOT mkae it larger than 1\n",
    "\n",
    "    for folders in folders_dataset:\n",
    "        images = Read3DImageFromFolder(folders)\n",
    "        category = ReadCategoryFromFolder(folders)\n",
    "        yield(images, 1)\n",
    "        # yield(images, folders)\n",
    "\n",
    "ds_3d_images = tf.data.Dataset.from_generator(gen, (tf.float32, tf.float32), (tf.TensorShape([IMG_HEIGHT, IMG_PX_SIZE, IMG_PX_SIZE, 1]), tf.TensorShape(())), args=([train_folder])).batch(BATCH_SIZE)\n",
    "\n",
    "# for i, (images, categories) in enumerate(ds_3d_images.take(300)):\n",
    "#     print(\"epoch 1) \" + str(i) + \")\", images.shape, categories.shape)\n",
    "# for i, (images, categories) in enumerate(ds_3d_images.take(300)):\n",
    "#     print(\"epoch 2) \" + str(i) + \")\", images.shape, categories.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 150, 150, 150, 1) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_61 (Conv3D)           (None, 150, 150, 150, 16) 448       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_37 (MaxPooling (None, 75, 75, 75, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 75, 75, 75, 16)    64        \n",
      "_________________________________________________________________\n",
      "conv3d_62 (Conv3D)           (None, 75, 75, 75, 32)    13856     \n",
      "_________________________________________________________________\n",
      "conv3d_63 (Conv3D)           (None, 75, 75, 75, 32)    27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_38 (MaxPooling (None, 18, 18, 18, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 18, 18, 18, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv3d_64 (Conv3D)           (None, 18, 18, 18, 64)    55360     \n",
      "_________________________________________________________________\n",
      "conv3d_65 (Conv3D)           (None, 18, 18, 18, 64)    110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_39 (MaxPooling (None, 9, 9, 9, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 9, 9, 9, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv3d_66 (Conv3D)           (None, 9, 9, 9, 128)      221312    \n",
      "_________________________________________________________________\n",
      "conv3d_67 (Conv3D)           (None, 9, 9, 9, 128)      442496    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_40 (MaxPooling (None, 4, 4, 4, 128)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 4, 4, 4, 128)      512       \n",
      "_________________________________________________________________\n",
      "conv3d_68 (Conv3D)           (None, 4, 4, 4, 256)      884992    \n",
      "_________________________________________________________________\n",
      "conv3d_69 (Conv3D)           (None, 4, 4, 4, 256)      1769728   \n",
      "_________________________________________________________________\n",
      "max_pooling3d_41 (MaxPooling (None, 2, 2, 2, 256)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 2, 2, 2, 256)      1024      \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 4,643,522\n",
      "Trainable params: 4,642,530\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.layers.Input((IMG_HEIGHT, IMG_PX_SIZE, IMG_PX_SIZE, 1))\n",
    "# input_layer = tf.keras.layers.MaxPool3D(pool_size=(1,2,2))(input_layer)\n",
    "\n",
    "conv_layer_1_1 = tf.keras.layers.Conv3D(filters=16, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(input_layer)\n",
    "pooling_layer_1 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2))(conv_layer_1_1)\n",
    "pooling_layer_1 = tf.keras.layers.BatchNormalization()(pooling_layer_1)\n",
    "\n",
    "conv_layer_2_1 = tf.keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(pooling_layer_1)\n",
    "conv_layer_2_2 = tf.keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(conv_layer_2_1)\n",
    "pooling_layer_2 = tf.keras.layers.MaxPool3D(pool_size=(4,4,4))(conv_layer_2_2)\n",
    "pooling_layer_2 = tf.keras.layers.BatchNormalization()(pooling_layer_2)\n",
    "\n",
    "conv_layer_3_1 = tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(pooling_layer_2)\n",
    "conv_layer_3_2 = tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(conv_layer_3_1)\n",
    "pooling_layer_3 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2))(conv_layer_3_2)\n",
    "pooling_layer_3 = tf.keras.layers.BatchNormalization()(pooling_layer_3)\n",
    "\n",
    "conv_layer_4_1 = tf.keras.layers.Conv3D(filters=128, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(pooling_layer_3)\n",
    "conv_layer_4_2 = tf.keras.layers.Conv3D(filters=128, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(conv_layer_4_1)\n",
    "pooling_layer_4 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2))(conv_layer_4_2)\n",
    "pooling_layer_4 = tf.keras.layers.BatchNormalization()(pooling_layer_4)\n",
    "\n",
    "conv_layer_5_1 = tf.keras.layers.Conv3D(filters=256, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(pooling_layer_4)\n",
    "conv_layer_5_2 = tf.keras.layers.Conv3D(filters=256, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(conv_layer_5_1)\n",
    "pooling_layer_5 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2))(conv_layer_5_2)\n",
    "pooling_layer_5 = tf.keras.layers.BatchNormalization()(pooling_layer_5)\n",
    "\n",
    "# conv_layer_6_1 = tf.keras.layers.Conv3D(filters=4, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(pooling_layer_5)\n",
    "# conv_layer_6_2 = tf.keras.layers.Conv3D(filters=4, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(conv_layer_6_1)\n",
    "# pooling_layer_6 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2))(conv_layer_6_2)\n",
    "# pooling_layer_6 = tf.keras.layers.BatchNormalization()(pooling_layer_6)\n",
    "\n",
    "# conv_layer_7_1 = tf.keras.layers.Conv3D(filters=4, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(pooling_layer_6)\n",
    "# conv_layer_7_2 = tf.keras.layers.Conv3D(filters=4, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(conv_layer_7_1)\n",
    "# pooling_layer_7 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2))(conv_layer_7_2)\n",
    "# pooling_layer_7 = tf.keras.layers.BatchNormalization()(pooling_layer_7)\n",
    "\n",
    "# conv_layer_8_1 = tf.keras.layers.Conv3D(filters=4, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(pooling_layer_7)\n",
    "# conv_layer_8_2 = tf.keras.layers.Conv3D(filters=4, kernel_size=(3, 3, 3), padding='SAME', activation='relu')(conv_layer_8_1)\n",
    "# pooling_layer_8 = tf.keras.layers.MaxPool3D(pool_size=(1,2,2))(conv_layer_8_2)\n",
    "# pooling_layer_8 = tf.keras.layers.BatchNormalization()(pooling_layer_8)\n",
    "\n",
    "flatten_layer = tf.keras.layers.Flatten()(pooling_layer_5)\n",
    "dense_layer_1 = tf.keras.layers.Dense(units=512, activation='relu')(flatten_layer)\n",
    "dense_layer_1 = tf.keras.layers.Dropout(0.4)(dense_layer_1)\n",
    "\n",
    "dense_layer_2 = tf.keras.layers.Dense(units=128, activation='relu')(dense_layer_1)\n",
    "dense_layer_2 = tf.keras.layers.Dropout(0.4)(dense_layer_2)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(units=2, activation=\"softmax\")(dense_layer_2)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 14 steps\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 929s 66s/step - loss: 3.0442 - acc: 0.4571\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 938s 67s/step - loss: 6.9036 - acc: 0.5286\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 937s 67s/step - loss: 17.2546 - acc: 0.5571\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 927s 66s/step - loss: 46.2137 - acc: 0.4857\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 962s 69s/step - loss: 132.8734 - acc: 0.4571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27346f46c88>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adadelta(lr=0.1),\n",
    "#     optimizer=tf.keras.optimizers.SGD(lr=0.1),\n",
    "    metrics=['acc'],\n",
    ")\n",
    "model.fit(\n",
    "    ds_3d_images, \n",
    "#     batch_size = BATCH_SIZE,\n",
    "    steps_per_epoch = int(0.75 * TRAIN_SET_SIZE / BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "#     validation_steps= int(0.25 * TRAIN_SET_SIZE / BATCH_SIZE),\n",
    "#     validation_split = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
